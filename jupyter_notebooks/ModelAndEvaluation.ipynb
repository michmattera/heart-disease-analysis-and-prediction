{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Model and evaluation Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Fit and evaluate a classification model to predict if a patient will suffer from heart disease or not.\n",
    "- Fulfil business requirement 2.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/heart.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Test set (features and target)\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* Feature importance plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Set up the Working Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "Define and confirm the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset and separate the target variable ('y') from the predictor variable ('X')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/heart.csv\"))\n",
    "\n",
    "# Separate predictors and target\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Pipeline with Data Cleaning and Feature Engineering already discussed in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "\n",
    "def pipeline_transformation():\n",
    "  pipeline_base = Pipeline([\n",
    "         (\"YeoJohnsonTransformer\", YeoJohnsonTransformer(variables=['age','cp','trestbps','chol','restecg','thalach','oldpeak','slope','ca','thal']) )\n",
    "  ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "pipeline_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Pipeline for Modelling and Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # Feat Scaling\n",
    "from sklearn.feature_selection import SelectFromModel # Feature Selection\n",
    "\n",
    "def pipeline_clf(model):\n",
    "  pipeline_base = Pipeline([\n",
    "       (\"scaler\",StandardScaler() ),\n",
    "        (\"feature_selection\", SelectFromModel(model)),\n",
    "        (\"model\", model),\n",
    "  ])\n",
    "\n",
    "  return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a custom class, used with permission from the Code Institute 'Churnometer' walkthrough project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model = pipeline_clf(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, )\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['target'], axis=1),\n",
    "    df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Combine train and test data into one DataFrame\n",
    "X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "y_combined = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = X_combined.duplicated()\n",
    "duplicates_exist = duplicates.any()\n",
    "\n",
    "if duplicates_exist:\n",
    "    print(\"Duplicates exist between train and test sets.\")\n",
    "else:\n",
    "    print(\"No duplicates found between train and test sets.\")\n",
    "\n",
    "# Print the shapes of train and test sets\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the number of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of duplicates in the train set\n",
    "train_duplicates = X_train[X_train.duplicated(keep='first')]\n",
    "num_train_duplicates = len(train_duplicates)\n",
    "\n",
    "# Check the number of duplicates in the test set\n",
    "test_duplicates = X_test[X_test.duplicated(keep='first')]\n",
    "num_test_duplicates = len(test_duplicates)\n",
    "\n",
    "print(\"Number of duplicates in the train set:\", num_train_duplicates)\n",
    "print(\"Number of duplicates in the test set:\", num_test_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are duplicates in train and test set:\n",
    "\n",
    "- Number of duplicates in the train set: 519\r",
    "- \n",
    "Number of duplicates in the test set: 4t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates from train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the test set\n",
    "X_test = X_test.drop_duplicates(keep='first')\n",
    "y_test = y_test.loc[X_test.index]\n",
    "\n",
    "# Check the shape of the test set after removing duplicates\n",
    "print(\"Test set shape after removing duplicates:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the train set\n",
    "X_train = X_train.drop_duplicates(keep='first')\n",
    "y_train = y_train.loc[X_train.index]\n",
    "\n",
    "# Check the shape of the train set after removing duplicates\n",
    "print(\"Train set shape after removing duplicates:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will identify the best Model and Hyperparameters to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models_quick_search = {\n",
    "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    \"XGBClassifier\":{},\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score\n",
    "quick_search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "quick_search.fit(X_train, y_train,\n",
    "                 scoring = make_scorer(recall_score, labels=[0], average=None),\n",
    "                 n_jobs=-1,\n",
    "                 cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the grid result the three best model with a mean score of maximum 0.75 were:\n",
    "\n",
    "- XGBClassifier with 0.75 mean score\n",
    "- GradientBoostingClassifier with 0.73 mean score\n",
    "- ExtraTreesClassifier with 0.73 mean score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main issue with the model and performance , was a performance and accuracy of 100 %. To resolve this issue developer tried different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach:\n",
    "- Developer checked that split of train and test set for issues, thinking the issue was data leakage.\n",
    "- Reload all sets , and tried to check for leaked data.\n",
    "- Issue was not solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second approach:\n",
    "- Developer found out that the main issue was the duplicates in the train and test set that were bringing the accuracy to 100 %.\n",
    "- Developer tried to eliminate just the duplicates from the test set.\n",
    "- Issue was not solved ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third approach:\n",
    "- To eliminate the duplicates from the test set and the train set.\n",
    "- The performance gave a result of 0.85 % resolving the 100 % issue.\n",
    "- Issue was solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensive search on the most suitable model to find the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "models_search = {\n",
    "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"XGBClassifier\": {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "    },\n",
    "    \"ExtraTreesClassifier\": {\n",
    "        \"model__n_estimators\": [20],\n",
    "        \"model__max_depth\": [5],\n",
    "        \"model__min_samples_split\": [2],\n",
    "        \"model__min_samples_leaf\": [1],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, make_scorer\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring =  make_scorer(recall_score, pos_label=1),\n",
    "           n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary above we note that the 2 best models are XGBClassifier and ExtraTreesClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier has been chosen as the best model.\n",
    "Choosing the best Hyperparameter as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the complete pipeline of the best model, including all preprocessing steps and the estimator itself, with the parameters set to their optimal values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classification_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_classification_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature inportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': X_train.columns[best_classification_pipeline['feature_selection'].get_support()],\n",
    "    'Importance': best_classification_pipeline['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# re-assign best_features order\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important features:\n",
    "\n",
    "- **cp** 0,75\n",
    "- **ca** and **thal** 0.20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
    "\n",
    "    prediction = pipeline.predict(X)\n",
    "\n",
    "    print('---  Confusion Matrix  ---')\n",
    "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
    "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
    "          ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('---  Classification Report  ---')\n",
    "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
    "\n",
    "    print(\"#### Test Set ####\\n\")\n",
    "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_classification_pipeline,\n",
    "                label_map= ['No disease', 'Disease'] \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set: \n",
    "\n",
    "- The report for the no disease is higher with a precision of the 0.87 and recall of 0.76.\n",
    "- The report for the disease lower with a precision of the 0.82 and recall of 0.91.\n",
    "- Mean accuracy of 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set: \n",
    "\n",
    "- The Report for the no disease is higher with a precision of the 0.90 and recall of 0.76.\n",
    "- The report for the disease lower with a precision of the 0.82 and recall of 0.93.\n",
    "- Mean accuracy of 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We create a new pipeline, using the best model and the best hyperparameters from the research above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def xgbclassifier_pipeline():\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'n_estimators': 100\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**hyperparameters)\n",
    "\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"YeoJohnsonTransformer\", YeoJohnsonTransformer(variables=['age','cp','trestbps','chol','restecg','thalach','oldpeak','slope','ca','thal'])),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(estimator=model)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "# Create the XGBoost pipeline with hyperparameters\n",
    "xgb_pipeline = xgbclassifier_pipeline()\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "confusion_matrix(y_true=xgb_pipeline.predict(X_test), y_pred=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=xgb_pipeline.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the classification report using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_pred=xgb_pipeline.predict(X_test), y_true=y_test, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame for easy plotting\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Plot the classification report using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_report.iloc[:-1, :3], annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Classification Report')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap and the report we can see that:\n",
    "\n",
    "- the precision for class 0 is 0.92\n",
    "- the precision for class 1 is 0.82\n",
    "- the recall for class 0 is 0.75\n",
    "- the recall for class 1 is 0.94\n",
    "- accuracy of 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Refit pipeline with best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite ml pipeline using best models, hyperparameters and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test set using just best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['target'], axis=1),\n",
    "    df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove again duplicates from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the test set\n",
    "X_test = X_test.drop_duplicates(keep='first')\n",
    "y_test = y_test.loc[X_test.index]\n",
    "\n",
    "# Check the shape of the test set after removing duplicates\n",
    "print(\"Test set shape after removing duplicates:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove again duplicates from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the train set\n",
    "X_train = X_train.drop_duplicates(keep='first')\n",
    "y_train = y_train.loc[X_train.index]\n",
    "\n",
    "# Check the shape of the train set after removing duplicates\n",
    "print(\"Train set shape after removing duplicates:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train.filter(best_features)\n",
    "X_test_filtered = X_test.filter(best_features)\n",
    "\n",
    "print(X_train_filtered.shape, y_train.shape, X_test_filtered.shape, y_test.shape)\n",
    "X_train_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaxamine performance of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def xgbclassifier_pipeline():\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'n_estimators': 100\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**hyperparameters)\n",
    "\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"YeoJohnsonTransformer\", YeoJohnsonTransformer(variables=['ca', 'cp', 'thal'])),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(estimator=model)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "# Create the XGBoost pipeline with hyperparameters\n",
    "xgb_pipeline = xgbclassifier_pipeline()\n",
    "xgb_pipeline.fit(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
    "\n",
    "    prediction = pipeline.predict(X)\n",
    "\n",
    "    print('---  Confusion Matrix  ---')\n",
    "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
    "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
    "          ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('---  Classification Report  ---')\n",
    "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train_filtered, y_train, X_test_filtered, y_test, pipeline, label_map):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    confusion_matrix_and_report(X_train_filtered, y_train, pipeline, label_map)\n",
    "\n",
    "    print(\"#### Test Set ####\\n\")\n",
    "    confusion_matrix_and_report(X_test_filtered, y_test, pipeline, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=xgb_pipeline.predict(X_test_filtered), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap for classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_pred=xgb_pipeline.predict(X_test_filtered), y_true=y_test, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame for easy plotting\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Plot the classification report using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_report.iloc[:-1, :3], annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Classification Report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training model just on best features the performance of both sets dropped of 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developer decided to not use these combination of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to refit pipeline with features Selection from previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['ca', 'cp', 'exang', 'oldpeak', 'thal', 'chol']\n",
    "best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter again the train and test set with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train.filter(best_features)\n",
    "X_test_filtered = X_test.filter(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_filtered.shape, y_train.shape, X_test_filtered.shape, y_test.shape)\n",
    "X_train_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaxamine performance of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def xgbclassifier_pipeline():\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'n_estimators': 100\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**hyperparameters)\n",
    "\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"YeoJohnsonTransformer\", YeoJohnsonTransformer(variables=['ca', 'cp', 'exang', 'oldpeak', 'thal', 'chol'])),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(estimator=model)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "# Create the XGBoost pipeline with hyperparameters\n",
    "xgb_pipeline = xgbclassifier_pipeline()\n",
    "xgb_pipeline.fit(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
    "\n",
    "    prediction = pipeline.predict(X)\n",
    "\n",
    "    print('---  Confusion Matrix  ---')\n",
    "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
    "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
    "          ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('---  Classification Report  ---')\n",
    "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train_filtered, y_train, X_test_filtered, y_test, pipeline, label_map):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    confusion_matrix_and_report(X_train_filtered, y_train, pipeline, label_map)\n",
    "\n",
    "    print(\"#### Test Set ####\\n\")\n",
    "    confusion_matrix_and_report(X_test_filtered, y_test, pipeline, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=xgb_pipeline.predict(X_test_filtered), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap for classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_pred=xgb_pipeline.predict(X_test_filtered), y_true=y_test, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame for easy plotting\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Plot the classification report using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_report.iloc[:-1, :3], annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Final Classification Report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance is again higher that the pipeline with just the 3 best features , found with features importance.\n",
    "- Performance is very similar to the pipeline with all features.\n",
    "\n",
    "From the heatmap and the report we can see the difference from the pipeline with all features are very small:\r\n",
    "- **the precision for class 0 from 0.92 to 0.90**\n",
    "- **the precision for class 1 from 0.82 remain the same**0- **the recall for class 0 from 0.75 to 0.76**\n",
    ".- **the recall for class 1 from 0.94 to 0.93**\n",
    "- **the accuracy from 0.86 to 0.85**\n",
    "\n",
    "\n",
    "Developer chose this as the best pipeline, and the be selection.res.f 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and push file to repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the following files:\n",
    "\n",
    "- Train set\n",
    "- Test set\n",
    "- Data cleaning and Feature Engineering pipeline\n",
    "- Modeling pipeline\n",
    "- features importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/predict_heart_disease/{version}'\n",
    "\n",
    "try:\n",
    "  os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_filtered\n",
    "X_test = X_test_filtered\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbclassifier_pipeline = xgb_pipeline\n",
    "xgbclassifier_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(value=xgbclassifier_pipeline,\n",
    "            filename=f\"{file_path}/xgbclassifier_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save plot showing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
